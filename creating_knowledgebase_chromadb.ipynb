{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a knowledge base using ChromaDB with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll load documents from a directory and create a vector store to build the knowledge base for a chatbot. I'll use the [ChromaDB](https://python.langchain.com/docs/integrations/vectorstores/chroma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to load documents from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from typing import Union\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#list of allowed file types\n",
    "ALLOWED_FILES = ['pdf', 'txt', 'docx', 'xml', 'html', 'md', 'json']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full documentation about directory loadder or file loaders can be found in the [langchain documentation](https://python.langchain.com/docs/integrations/document_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to load a single file, HTML\n",
    "def load_html(file_path: str):\n",
    "    '''\n",
    "        function to load html files form the dir\n",
    "\n",
    "        params:\n",
    "            file_path: str: path of the file with its name\n",
    "\n",
    "        returns:\n",
    "            Doc object\n",
    "    '''\n",
    "\n",
    "    #import HTML loader\n",
    "    from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "    #prepare loader\n",
    "    print(f\"Loading HTML file '{file_path}'...\")\n",
    "    loader = UnstructuredHTMLLoader(file_path)\n",
    "    #load document\n",
    "    doc = loader.load()\n",
    "    print(f\"Loaded HTML file '{doc.metadata['filename']}'\")\n",
    "    #return document\n",
    "    return doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load a directory\n",
    "def load_directory(path:str, file_types: Union[str, list]):\n",
    "    '''\n",
    "        function to load documents from a directory\n",
    "    \n",
    "        params:\n",
    "            path: str: path pof the dir to load documents from\n",
    "            file_types: (str or list of strings): file formats to load e.g. .pdf, .docx etc\n",
    "\n",
    "        returns:\n",
    "            Docloader object\n",
    "\n",
    "    '''\n",
    "\n",
    "    # convert file_types to a list if it is a string\n",
    "    if isinstance(file_types, str):\n",
    "        file_types = [file_types]\n",
    "\n",
    "    # if file_types is not in the allowed files list, raise an error\n",
    "    for fl in file_types:\n",
    "        if fl not in ALLOWED_FILES:\n",
    "            raise ValueError(f\"File type {fl} is not allowed. Allowed file types are: {ALLOWED_FILES}\")\n",
    "\n",
    "    #list to store all loader objects\n",
    "    docs_list = []\n",
    "    #for each file type in the file_types parameter\n",
    "    for fl in file_types:\n",
    "        #load docs\n",
    "        print(f\"Loading {fl} files...\")\n",
    "        try:\n",
    "            if fl != 'html':\n",
    "                loader = DirectoryLoader(\n",
    "                            path,\n",
    "                            glob=f\"**/*.{fl}\",\n",
    "                            show_progress=True,\n",
    "                            use_multithreading=True,\n",
    "                            recursive=True\n",
    "                        )\n",
    "                docs = loader.load()\n",
    "                print(f\"Loaded {len(docs)} {fl} files\")\n",
    "                #add into the list of loader objects\n",
    "                docs_list+=docs\n",
    "            else:\n",
    "                html_list = glob.glob(os.path.join(path, '*.html'))\n",
    "                docs = []\n",
    "                for ht in html_list:\n",
    "                    #add into the list of loader objects\n",
    "                    docs_list+=load_html(ht)\n",
    "                    # docs.append(load_html(ht))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {fl} files: {e}\")\n",
    "    \n",
    "    return docs_list\n",
    "    #return the final loaded list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A processor function to process the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import document_loaders\n",
    "\n",
    "\n",
    "def process_docs(docs: document_loaders):\n",
    "    '''\n",
    "        function to split text into sentences and words\n",
    "\n",
    "        parameters:\n",
    "            docs: list of document objects\n",
    "\n",
    "        returns:\n",
    "            texts: list of text objects\n",
    "    '''\n",
    "\n",
    "    #load text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter()\n",
    "    #split documents\n",
    "    print('Splitting documents...')\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving documents to a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  setup sqlite3 for linux based OS, for windows it's not needed, comment these lines for windows\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma DB max batch size: 41666\n"
     ]
    }
   ],
   "source": [
    "# # get chroma db max batch size\n",
    "from chromadb import Client\n",
    "chromadb_client = Client()\n",
    "chroma_batch_size = chromadb_client.max_batch_size\n",
    "print(f\"Chroma DB max batch size: {chroma_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain import embeddings, text_splitter\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import time\n",
    "\n",
    "def store_db(texts: text_splitter, embeddings: embeddings, persist_directory: Optional[str], persist: Optional[bool]):\n",
    "    '''\n",
    "        function to store the database\n",
    "\n",
    "        parameters:\n",
    "            texts: list of text objects\n",
    "            embeddings: embeddings to apply\n",
    "            persist_directory: directory to save the database\n",
    "            persist: whether to persist the database or not\n",
    "\n",
    "        returns:\n",
    "            vector_store: vector store object\n",
    "    '''\n",
    "\n",
    "    vector_db = None\n",
    "    def store(text):\n",
    "\n",
    "        #if persist is true and persist_directory is not None or persist is false and persist_directory is not None\n",
    "        if (persist and persist_directory is not None) or (not persist and persist_directory is not None):\n",
    "            #store texts into directory\n",
    "            print(f\"Storing database in directory '{persist_directory}'...\")\n",
    "\n",
    "            vector_db = vector_store.from_documents(\n",
    "                                            documents=text, \n",
    "                                            embedding=embeddings,\n",
    "                                            persist_directory=persist_directory\n",
    "                                        )\n",
    "            \n",
    "        #else if persist is true and persist_directory is None\n",
    "        elif persist and persist_directory is None:\n",
    "            #store texts into memory\n",
    "            raise Exception(\"Persist directory must be specified to persist the database\")\n",
    "        #else if persist is false and persist_directory is None\n",
    "        elif not persist and persist_directory is None:\n",
    "            print(\"Storing database in memory...\")\n",
    "            #store texts into memory\n",
    "            vector_db = vector_store.from_documents(documents=texts, \n",
    "                                    embedding=embeddings)\n",
    "            \n",
    "        return vector_db\n",
    "\n",
    "\n",
    "    print('total docs to be stored: ', len(texts))\n",
    "\n",
    "    #load vector store\n",
    "    vector_store = Chroma()\n",
    "\n",
    "    #if the number of texts is greater than the chroma db max batch size\n",
    "    if len(texts) > chroma_batch_size:\n",
    "        #split texts into batches\n",
    "        texts = [texts[i:i+chroma_batch_size] for i in range(0, len(texts), chroma_batch_size)]\n",
    "        print('total batches: ', len(texts))\n",
    "\n",
    "        #for each batch in texts\n",
    "        # store batch\n",
    "        # keep count of the number of batches\n",
    "        # after every 3rd batch sleep for 1.5 minutes\n",
    "        # this is to prevent the chroma db from crashing\n",
    "        for i, batch in enumerate(texts):\n",
    "            #store batch\n",
    "            vector_db = store(batch)\n",
    "            time.sleep(1.5)\n",
    "            if i % 3 == 0:\n",
    "                print('sleeping for 90 seconds...')\n",
    "                time.sleep(90)\n",
    "    else:\n",
    "        #store texts\n",
    "        vector_db = store(texts)\n",
    "\n",
    "    \n",
    "    if vector_db is None:\n",
    "        raise Exception('Database could not be stored')\n",
    "    \n",
    "    return vector_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function to ingest documents from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some accessory modules from langchain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "import argparse\n",
    "from typing import Union, Optional\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "open_ai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ingest(\n",
    "            path: str,\n",
    "            file_types: str = 'pdf, docx, html, txt, xml, md, json',\n",
    "            embeddings=OpenAIEmbeddings(\n",
    "                                        openai_api_key=open_ai_api_key,\n",
    "                                        show_progress_bar=True\n",
    "                                    ),\n",
    "            persist_directory: Optional[str]='db',\n",
    "            persist: Optional[bool]=True\n",
    "        ):\n",
    "    '''\n",
    "        function to ingest documents\n",
    "\n",
    "        parameters:\n",
    "            path: str: path of the dir to load documents from\n",
    "            file_types: str: file formats to load e.g. 'pdf, docx, md, txt'  etc\n",
    "            embeddings: embeddings to apply\n",
    "            persist_directory: directory to save the database\n",
    "            persist: whether to persist the database or not\n",
    "\n",
    "        returns:\n",
    "            vector_store: vector store object\n",
    "    '''\n",
    "\n",
    "    file_types = file_types.split(',')\n",
    "\n",
    "    file_types = [fl.strip() for fl in file_types if fl.strip() != '']\n",
    "\n",
    "    # print(file_types)\n",
    "    print('File types: ', file_types)\n",
    "\n",
    "    print('Loading documents from directory...', path)\n",
    "\n",
    "    #load documents\n",
    "    docs = load_directory(path, file_types)\n",
    "    print(f'Loaded {len(docs)} documents from directory: ', path, ' Successfully')\n",
    "    #process documents\n",
    "    texts = process_docs(docs)\n",
    "    #store database\n",
    "    vector_store = store_db(texts, embeddings, persist_directory, persist)\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test the directory ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The db_files dir contains a book related to bioinformatics and an HTML file of bioinformatics wikipedia. Let's ingest that and then we'll query the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = ingest(\n",
    "                'db_files',\n",
    "                file_types='html, txt, pdf, docx, md, xml, json',\n",
    "                embeddings=OpenAIEmbeddings(\n",
    "                                        openai_api_key=open_ai_api_key,\n",
    "                                        show_progress_bar=True\n",
    "                                    ),\n",
    "                persist_directory='db2',\n",
    "                persist=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can use the following method in a .py script**\n",
    "\n",
    "It can also be run with in scheduled to automate directory loading and ingestion of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(description='Ingest documents into a vector store')\n",
    "#     parser.add_argument('path', type=str, help='path of the dir to load documents from')\n",
    "#     parser.add_argument('file_types', type=str, help='file formats to load e.g. .pdf, .docx etc')\n",
    "#     parser.add_argument('persist_directory', type=str, help='directory to save the database')\n",
    "#     parser.add_argument('persist', type=bool, help='whether to persist the database or not')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     #embeddings\n",
    "#     embeddings = OpenAIEmbeddings(openai_api_key=open_ai_api_key )\n",
    "\n",
    "#     #ingest documents\n",
    "#     store = ingest(args.path, args.file_types.strip(), embeddings, args.persist_directory, args.persist)\n",
    "\n",
    "#     print('Database stored successfully', store.get()['metadatas'][0]['source'])\n",
    "\n",
    "#     print('Exiting...')\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
