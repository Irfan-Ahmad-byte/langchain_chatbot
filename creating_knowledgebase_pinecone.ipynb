{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a knowledge base using Pinecone with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll load documents from a directory and create a vector store to build the knowledge base for a chatbot. I'll use the [Langchain Pinecone](https://python.langchain.com/docs/integrations/vectorstores/pinecone), [Pinecone official](https://www.pinecone.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to load documents from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from typing import Union\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#list of allowed file types\n",
    "ALLOWED_FILES = ['pdf', 'txt', 'docx', 'xml', 'html', 'md', 'json']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full documentation about directory loadder or file loaders can be found in the [langchain documentation](https://python.langchain.com/docs/integrations/document_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to load a single file, HTML\n",
    "def load_html(file_path: str):\n",
    "    '''\n",
    "        function to load html files form the dir\n",
    "\n",
    "        params:\n",
    "            file_path: str: path of the file with its name\n",
    "\n",
    "        returns:\n",
    "            Doc object\n",
    "    '''\n",
    "\n",
    "    #import HTML loader\n",
    "    from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "    #prepare loader\n",
    "    print(f\"Loading HTML file '{file_path}'...\")\n",
    "    loader = UnstructuredHTMLLoader(file_path)\n",
    "    #load document\n",
    "    doc = loader.load()\n",
    "    print(f\"Loaded HTML file '{doc.metadata['filename']}'\")\n",
    "    #return document\n",
    "    return doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load a directory\n",
    "def load_directory(path:str, file_types: Union[str, list]):\n",
    "    '''\n",
    "        function to load documents from a directory\n",
    "    \n",
    "        params:\n",
    "            path: str: path pof the dir to load documents from\n",
    "            file_types: (str or list of strings): file formats to load e.g. .pdf, .docx etc\n",
    "\n",
    "        returns:\n",
    "            Docloader object\n",
    "\n",
    "    '''\n",
    "\n",
    "    # convert file_types to a list if it is a string\n",
    "    if isinstance(file_types, str):\n",
    "        file_types = [file_types]\n",
    "\n",
    "    # if file_types is not in the allowed files list, raise an error\n",
    "    for fl in file_types:\n",
    "        if fl not in ALLOWED_FILES:\n",
    "            raise ValueError(f\"File type {fl} is not allowed. Allowed file types are: {ALLOWED_FILES}\")\n",
    "\n",
    "    #list to store all loader objects\n",
    "    docs_list = []\n",
    "    #for each file type in the file_types parameter\n",
    "    for fl in file_types:\n",
    "        #load docs\n",
    "        print(f\"Loading {fl} files...\")\n",
    "        try:\n",
    "            if fl != 'html':\n",
    "                loader = DirectoryLoader(\n",
    "                            path,\n",
    "                            glob=f\"**/*.{fl}\",\n",
    "                            show_progress=True,\n",
    "                            use_multithreading=True,\n",
    "                            recursive=True\n",
    "                        )\n",
    "                docs = loader.load()\n",
    "                print(f\"Loaded {len(docs)} {fl} files\")\n",
    "                #add into the list of loader objects\n",
    "                docs_list+=docs\n",
    "            else:\n",
    "                html_list = glob.glob(os.path.join(path, '*.html'))\n",
    "                docs = []\n",
    "                for ht in html_list:\n",
    "                    #add into the list of loader objects\n",
    "                    docs_list+=load_html(ht)\n",
    "                    # docs.append(load_html(ht))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {fl} files: {e}\")\n",
    "    \n",
    "    return docs_list\n",
    "    #return the final loaded list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A processor function to process the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import document_loaders\n",
    "\n",
    "\n",
    "def process_docs(docs: document_loaders):\n",
    "    '''\n",
    "        function to split text into sentences and words\n",
    "\n",
    "        parameters:\n",
    "            docs: list of document objects\n",
    "\n",
    "        returns:\n",
    "            texts: list of text objects\n",
    "    '''\n",
    "\n",
    "    #load text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter()\n",
    "    #split documents\n",
    "    print('Splitting documents...')\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving documents to a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  setup sqlite3 for linux based OS, for windows it's not needed, comment these lines for windows\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "# load the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "open_ai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api = os.getenv('PINECONE_API_KEY')\n",
    "pinecone_env = os.getenv('PINECONE_ENV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/Documents/arabic_law_assistant/venv/lib/python3.9/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=pinecone_api,\n",
    "    environment=pinecone_env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an database aka Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"my-knowledgebase\"\n",
    "\n",
    "# First, check if our index already exists. If it doesn't, we create it\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "      name=index_name,\n",
    "      metric='cosine',\n",
    "      dimension=1536  # based on the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my-knowledgebase']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list indexes to see if it exists\n",
    "\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain import embeddings, text_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import time\n",
    "\n",
    "def store_db(texts: text_splitter, embeddings: embeddings, index_name: str):\n",
    "    '''\n",
    "        function to store the database\n",
    "\n",
    "        parameters:\n",
    "            texts: list of text objects\n",
    "            embeddings: embeddings to apply\n",
    "            persist_directory: directory to save the database\n",
    "            persist: whether to persist the database or not\n",
    "\n",
    "        returns:\n",
    "            vector_store: vector store object\n",
    "    '''\n",
    "\n",
    "    # First, check if our index already exists. If it doesn't, we create it\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        # we create a new index\n",
    "        pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=1536  # based on the model\n",
    "    )\n",
    "\n",
    "    vector_db = None\n",
    "\n",
    "    vector_db = Pinecone.from_documents(texts, embeddings, index_name=index_name)\n",
    "    \n",
    "\n",
    "    print('total docs to be stored: ', len(texts))\n",
    "\n",
    "    if vector_db is None:\n",
    "        raise Exception('Database could not be stored')\n",
    "    \n",
    "    return vector_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=open_ai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function to ingest documents from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Union, Optional\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ingest(\n",
    "            path: str,\n",
    "            file_types: str = 'pdf, docx, html, txt, xml, md, json',\n",
    "            embeddings=OpenAIEmbeddings(\n",
    "                                        openai_api_key=open_ai_api_key,\n",
    "                                        show_progress_bar=True\n",
    "                                    )\n",
    "        ):\n",
    "    '''\n",
    "        function to ingest documents\n",
    "\n",
    "        parameters:\n",
    "            path: str: path of the dir to load documents from\n",
    "            file_types: str: file formats to load e.g. 'pdf, docx, md, txt'  etc\n",
    "            embeddings: embeddings to apply\n",
    "            persist_directory: directory to save the database\n",
    "            persist: whether to persist the database or not\n",
    "\n",
    "        returns:\n",
    "            vector_store: vector store object\n",
    "    '''\n",
    "\n",
    "    file_types = file_types.split(',')\n",
    "\n",
    "    file_types = [fl.strip() for fl in file_types if fl.strip() != '']\n",
    "\n",
    "    # print(file_types)\n",
    "    print('File types: ', file_types)\n",
    "\n",
    "    print('Loading documents from directory...', path)\n",
    "\n",
    "    #load documents\n",
    "    docs = load_directory(path, file_types)\n",
    "    print(f'Loaded {len(docs)} documents from directory: ', path, ' Successfully')\n",
    "    #process documents\n",
    "    texts = process_docs(docs)\n",
    "    #store database\n",
    "    vector_store = store_db(texts, embeddings, index_name=index_name)\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test the directory ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The db_files dir contains a book related to bioinformatics and an HTML file of bioinformatics wikipedia. Let's ingest that and then we'll query the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File types:  ['html', 'txt', 'pdf', 'docx', 'md', 'xml', 'json']\n",
      "Loading documents from directory... db_files\n",
      "Loading html files...\n",
      "Loading HTML file 'db_files/Bioinformatics - Wikipedia.html'...\n",
      "Error loading html files: 'list' object has no attribute 'metadata'\n",
      "Loading txt files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 txt files\n",
      "Loading pdf files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:13<00:00, 73.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 pdf files\n",
      "Loading docx files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 docx files\n",
      "Loading md files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 md files\n",
      "Loading xml files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 xml files\n",
      "Loading json files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 json files\n",
      "Loaded 1 documents from directory:  db_files  Successfully\n",
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total docs to be stored:  186\n"
     ]
    }
   ],
   "source": [
    "store = ingest(\n",
    "                'db_files',\n",
    "                file_types='html, txt, pdf, docx, md, xml, json',\n",
    "                embeddings=OpenAIEmbeddings(\n",
    "                                        openai_api_key=open_ai_api_key,\n",
    "                                        show_progress_bar=True\n",
    "                                    ),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.pinecone.Pinecone at 0x7f4c044abf70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can use the following method in a .py script**\n",
    "\n",
    "It can also be run with in scheduled to automate directory loading and ingestion of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(description='Ingest documents into a vector store')\n",
    "#     parser.add_argument('path', type=str, help='path of the dir to load documents from')\n",
    "#     parser.add_argument('file_types', type=str, help='file formats to load e.g. .pdf, .docx etc')\n",
    "#     parser.add_argument('index_name', type=str, help='name of the pinecone index to interact with')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     #embeddings\n",
    "#     embeddings = OpenAIEmbeddings(openai_api_key=open_ai_api_key )\n",
    "\n",
    "#     #ingest documents\n",
    "#     store = ingest(args.path, args.file_types.strip(), embeddings, index_name=args.index_name.strip())\n",
    "\n",
    "#     print('Database stored successfully', store.get()['metadatas'][0]['source'])\n",
    "\n",
    "#     print('Exiting...')\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
